{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "909d0c77-5a46-4dae-95b0-a08b855100b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Current user\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get().split(\"@\")[0]\n",
    "print(current_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57883775-badf-4765-80db-6acfc13cfe15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract part before '@'\n",
    "user_name = current_user.split(\"@\")[0]\n",
    "\n",
    "print(\"Full email:\", current_user)\n",
    "print(\"User name:\", user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7dfce8b-f0f6-4bf8-85d2-7995687a8501",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6474decc-e1a9-4a80-86c2-2c82a3173ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# CONFIG\n",
    "# ============================\n",
    "CATALOG = \"dbacademy\"\n",
    "SCHEMA = user_name\n",
    "BASELINE_TABLE = f\"{CATALOG}.{SCHEMA}.baseline_data\"\n",
    "CURRENT_TABLE  = f\"{CATALOG}.{SCHEMA}.current_data\"\n",
    "OUTPUT_TABLE   = f\"{CATALOG}.{SCHEMA}.drift_summary\"\n",
    "\n",
    "NUMERIC_COLS = [\"feature_num\"]\n",
    "CATEGORICAL_COLS = [\"feature_cat\"]\n",
    "ALPHA = 0.05   # significance level\n",
    "MODEL_NAME = f\"{CATALOG}.{SCHEMA}.demo_drift_model\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ad5efa-cd63-4b4b-9fe3-edcb1413c1a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# SYNTHETIC DATA GENERATION\n",
    "# ============================\n",
    "\n",
    "# Baseline numeric + categorical\n",
    "np.random.seed(42)\n",
    "baseline_num = np.random.normal(loc=50, scale=10, size=1000)\n",
    "baseline_cat = np.random.choice([\"A\", \"B\", \"C\"], size=1000, p=[0.6, 0.3, 0.1])\n",
    "baseline_df = pd.DataFrame({\"feature_num\": baseline_num, \"feature_cat\": baseline_cat})\n",
    "\n",
    "# Current (simulate drift by shifting mean + distribution)\n",
    "current_num = np.random.normal(loc=60, scale=12, size=1000)   # shifted mean\n",
    "current_cat = np.random.choice([\"A\", \"B\", \"C\"], size=1000, p=[0.4, 0.2, 0.4])  # shifted probs\n",
    "current_df = pd.DataFrame({\"feature_num\": current_num, \"feature_cat\": current_cat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c592f52-a033-4d5d-a859-05179ed45ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save to Unity Catalog as Delta tables\n",
    "spark.createDataFrame(baseline_df).write.mode(\"overwrite\").saveAsTable(BASELINE_TABLE)\n",
    "spark.createDataFrame(current_df).write.mode(\"overwrite\").saveAsTable(CURRENT_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecec8a89-ba2f-49b1-873c-79e4266f245c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# DRIFT FUNCTIONS\n",
    "# ============================\n",
    "\n",
    "def population_stability_index(expected, actual, buckets=10):\n",
    "    breakpoints = np.percentile(expected, np.linspace(0, 100, buckets + 1))\n",
    "    expected_percents = np.histogram(expected, bins=breakpoints)[0] / len(expected)\n",
    "    actual_percents = np.histogram(actual, bins=breakpoints)[0] / len(actual)\n",
    "    psi = np.sum((expected_percents - actual_percents) * np.log((expected_percents + 1e-6) / (actual_percents + 1e-6)))\n",
    "    return psi\n",
    "\n",
    "def chi_square_test(expected, actual):\n",
    "    exp_counts = expected.value_counts()\n",
    "    act_counts = actual.value_counts()\n",
    "    all_categories = set(exp_counts.index).union(set(act_counts.index))\n",
    "    exp_aligned = exp_counts.reindex(all_categories, fill_value=0)\n",
    "    act_aligned = act_counts.reindex(all_categories, fill_value=0)\n",
    "    chi2, p, _, _ = stats.chi2_contingency([exp_aligned, act_aligned])\n",
    "    return chi2, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d784e5-ec0d-4393-b3d0-d4c4b9600afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# DRIFT TESTS\n",
    "# ============================\n",
    "\n",
    "baseline_pd = baseline_df\n",
    "current_pd  = current_df\n",
    "results = []\n",
    "\n",
    "# Numeric: KS + PSI\n",
    "for col in NUMERIC_COLS:\n",
    "    ks_stat, ks_p = stats.ks_2samp(baseline_pd[col], current_pd[col])\n",
    "    psi_val = population_stability_index(baseline_pd[col], current_pd[col])\n",
    "    results.append((col, \"numeric\", f\"KS={ks_stat:.4f}, p={ks_p:.4f}\", f\"PSI={psi_val:.4f}\",\n",
    "                    \"DRIFT\" if (ks_p < ALPHA and psi_val > 0.1) else \"NO_DRIFT\"))\n",
    "\n",
    "# Categorical: Chi-square\n",
    "for col in CATEGORICAL_COLS:\n",
    "    chi2, chi_p = chi_square_test(baseline_pd[col], current_pd[col])\n",
    "    results.append((col, \"categorical\", f\"Chi2={chi2:.4f}, p={chi_p:.4f}\", None,\n",
    "                    \"DRIFT\" if chi_p < ALPHA else \"NO_DRIFT\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0e3e4b-b66d-4654-b899-301093ded419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write drift results\n",
    "results_df = pd.DataFrame(results, columns=[\"feature\", \"type\", \"test_result\", \"effect_size\", \"drift_flag\"])\n",
    "spark_results = spark.createDataFrame(results_df)\n",
    "spark_results.write.mode(\"overwrite\").saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "display(spark.table(OUTPUT_TABLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ddb9c5f-6fd8-43c4-b04a-1a724448a8c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# AUTOMATED RETRAINING\n",
    "# ============================\n",
    "\n",
    "# Check if any drift detected\n",
    "drift_detected = results_df[\"drift_flag\"].eq(\"DRIFT\").any()\n",
    "\n",
    "if drift_detected:\n",
    "    print(\"ðŸš¨ Drift detected! Triggering automated retraining...\")\n",
    "\n",
    "    # Synthetic classification dataset for retraining demo\n",
    "    X, y = make_classification(n_samples=2000, n_features=5, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train logistic regression\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "    # Log to MLflow\n",
    "    mlflow.set_experiment(\"/Shared/drift_retraining_demo\")\n",
    "    # âœ… Infer model signature (input + output schema)\n",
    "    signature = infer_signature(X_train, model.predict_proba(X_train))\n",
    "\n",
    "    # âœ… Create example input (small pandas DataFrame)\n",
    "    example_input = pd.DataFrame(X_train[:5], columns=[f\"f{i}\" for i in range(X_train.shape[1])])\n",
    "\n",
    "    # Log to MLflow with signature + example\n",
    "    mlflow.set_experiment(\"/Shared/drift_retraining_demo\")\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_metric(\"AUC\", auc)\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=MODEL_NAME,\n",
    "            signature=signature,\n",
    "            input_example=example_input\n",
    "        )\n",
    "\n",
    "    print(f\"âœ… New model trained, logged, and registered with AUC={auc:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"âœ… No drift detected. Skipping retraining.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "drift_detection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
